{
    "piqa": {
        "test_path": "datasets/physics/piqa/piqa_valid.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_piqa_sft",
        "answer_extraction_fn": "extract_two_choice_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTCSBenchAssertionPrompt"
    },
    "scibench-physics":{
        "test_path": "datasets/physics/scibench_physics/scibench_physics.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_scibench_sft",
        "answer_extraction_fn": "extract_scibench_sft_answer",
        "eval_fn": "eval_scibench",
        "few_shot_prompt": "SFTScibenchPrompt"
    },
    "scibench-chemistry":{
        "test_path": "datasets/chemistry/scibench_chemistry/scibench_chemistry.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_scibench_sft",
        "answer_extraction_fn": "extract_scibench_sft_answer",
        "eval_fn": "eval_scibench",
        "few_shot_prompt": "SFTScibenchPrompt"
    },
    "scibench-math":{
        "test_path": "datasets/scibench/scibench_math.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_scibench_sft",
        "answer_extraction_fn": "extract_scibench_sft_answer",
        "eval_fn": "eval_scibench",
        "few_shot_prompt": "SFTScibenchPrompt"
    },
    "olympic-arena-astronomy":{
        "test_path": "datasets/olympic_arena_en_text/Astronomy.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-biology":{
        "test_path": "datasets/olympic_arena_en_text/Biology.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-chemistry":{
        "test_path": "datasets/olympic_arena_en_text/Chemistry.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-geography":{
        "test_path": "datasets/olympic_arena_en_text/Geography.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-math":{
        "test_path": "datasets/olympic_arena_en_text/Math.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-physics":{
        "test_path": "datasets/olympic_arena_en_text/Physics.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    }
}