{
    "mmlu":{
        "test_path": "datasets/mmlu/mmlu.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_mmlu_sft",
        "answer_extraction_fn": "extract_mmlu_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTMMLUPrompt"
    },
    "mmlu_pro":{
        "test_path": "datasets/mmlu_pro/test.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_mmlu_pro",
        "answer_extraction_fn": "extract_mmlu_pro_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTSuperGPQAPrompt"
    },
    "gpqa_diamond":{
        "test_path": "datasets/gpqa/gpqa_diamond.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_gpqa_sft",
        "answer_extraction_fn": "extract_gpqa_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTMMLUPrompt"
    },
    "gpqa_main":{
        "test_path": "datasets/gpqa/gpqa_main.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_gpqa_sft",
        "answer_extraction_fn": "extract_gpqa_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTMMLUPrompt"
    },
    "super_gpqa":{
        "test_path": "datasets/super_gpqa/super_gpqa.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_super_gpqa",
        "answer_extraction_fn": "extract_super_gpqa_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTSuperGPQAPrompt"
    },
    "gsm8k": {
        "test_path": "datasets/gsm8k/test.jsonl",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_gsm8k_test",
        "answer_extraction_fn": "extract_math_few_shot_cot_answer",
        "eval_fn": "eval_math",
        "few_shot_prompt": "SFTMATH0ShotPrompt"
    },
    "math": {
        "test_path": "datasets/math/test.jsonl",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_math_test",
        "answer_extraction_fn": "extract_math_few_shot_cot_answer",
        "eval_fn": "eval_math",
        "few_shot_prompt": "SFTMATH0ShotPrompt"
    },
    "math-500": {
        "test_path": "datasets/math500/test.jsonl",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_math500",
        "answer_extraction_fn": "extract_math_few_shot_cot_answer",
        "eval_fn": "eval_math",
        "few_shot_prompt": "SFTMATH0ShotPrompt"
    },
    "ChemBench-multi-choise": {
        "test_path": "datasets/chemistry/chembench/chemistry_multi_choise.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_ChemBench_multi_choise_sft",
        "answer_extraction_fn": "extract_adaptive_mc_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTSuperGPQAPrompt"
    },
    "ChemBench-str-match": {
        "test_path": "datasets/chemistry/chembench/chemistry_str_match.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_ChemBench_str_match",
        "answer_extraction_fn": "extract_math_few_shot_cot_answer",
        "eval_fn": "eval_math",
        "few_shot_prompt": "SFTMATH0ShotPrompt"
    },
    "cs-bench-assertion-questions": {
        "test_path": "datasets/computer_science/cs_bench/cs_bench_assertion.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_cs_bench_assertion_sft",
        "answer_extraction_fn": "extract_two_choice_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTCSBenchAssertionPrompt"
    },
    "cs-bench-multiple-choice-questions":{
        "test_path": "datasets/computer_science/cs_bench/cs_bench_multiple_choice.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_cs_bench_multiple_choice_sft",
        "answer_extraction_fn": "extract_mmlu_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTMMLUPrompt"
    },
    "medqa-us": {
        "test_path": "datasets/medicine/medqa_US/test.jsonl",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_medqa_sft",
        "answer_extraction_fn": "extract_five_choice_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTMedQAPrompt"
    },
    "medmcqa": {
        "test_path": "datasets/medicine/medmcqa/medmcqa.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_medmcqa_sft",
        "answer_extraction_fn": "extract_mmlu_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTMMLUPrompt"
    },
    "PubMedQA":{
        "test_path": "datasets/medicine/pubmedqa/testset.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_pubmedqa_sft",
        "answer_extraction_fn": "extract_three_choice_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTPubmedQAPrompt"
    },
    "piqa": {
        "test_path": "datasets/physics/piqa/piqa_valid.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_piqa_sft",
        "answer_extraction_fn": "extract_two_choice_sft_answer",
        "eval_fn": "eval_mmlu_stem",
        "few_shot_prompt": "SFTCSBenchAssertionPrompt"
    },
    "scibench-physics":{
        "test_path": "datasets/physics/scibench_physics/scibench_physics.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_scibench_sft",
        "answer_extraction_fn": "extract_scibench_sft_answer",
        "eval_fn": "eval_scibench",
        "few_shot_prompt": "SFTScibenchPrompt"
    },
    "scibench-chemistry":{
        "test_path": "datasets/chemistry/scibench_chemistry/scibench_chemistry.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_scibench_sft",
        "answer_extraction_fn": "extract_scibench_sft_answer",
        "eval_fn": "eval_scibench",
        "few_shot_prompt": "SFTScibenchPrompt"
    },
    "scibench-math":{
        "test_path": "datasets/scibench/scibench_math.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_scibench_sft",
        "answer_extraction_fn": "extract_scibench_sft_answer",
        "eval_fn": "eval_scibench",
        "few_shot_prompt": "SFTScibenchPrompt"
    },
    "olympic-arena-astronomy":{
        "test_path": "datasets/olympic_arena_en_text/Astronomy.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-biology":{
        "test_path": "datasets/olympic_arena_en_text/Biology.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-chemistry":{
        "test_path": "datasets/olympic_arena_en_text/Chemistry.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-geography":{
        "test_path": "datasets/olympic_arena_en_text/Geography.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-math":{
        "test_path": "datasets/olympic_arena_en_text/Math.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    },
    "olympic-arena-physics":{
        "test_path": "datasets/olympic_arena_en_text/Physics.json",
        "language": "en",
        "tasks": [
            "sft"
        ],
        "process_fn": "process_olympic_arena_sft",
        "answer_extraction_fn": "extract_olympic_arena_sft_answer",
        "eval_fn": "eval_olympic_arena",
        "few_shot_prompt": "SFTOlympicArenaPrompt"
    }
}